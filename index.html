<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="google-site-verification" content="2IJJySc3dA1kV3YKWmyeiucsLWFv3_EXELkFcmUmIpQ" />
    <meta name="description" content="ClinIQLink 2025 - Evaluating the ability of generative models to produce factually accurate medical information for the BioNLP Workshop at ACL 2025.">
    <meta name="keywords" content="ClinIQLink, BioNLP Workshop, ACL 2025, generative models, medical QA, LLM hallucinations, ClinIQLink 2025, LLM Evaluation, Medical Knowledge Retrieval, Biomedical NLP, ACL 2025 Workshop, Generative AI Models, LLM Hallucinations, Medical QA Datasets, ClinIQLink Shared Task, Semantic Answer Matching, Factual Accuracy in AI">
    <meta name="author" content="Brandon Colelough">    
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ClinIQLink 2025 - LLM Lie Detector Test</title>
    <link rel="icon" type="image/svg+xml" href="Images/ClinIQLink-logo.svg">
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #ffffff;
            color: #212121;
            box-sizing: border-box;
        }
        header {
            background-color: #0071bc;
            color: white;
            padding: 15px;
            text-align: center;
        }
        main {
            padding: 10px;
        }
        h1 {
            color: white;
        }
        h2, h3 {
            color: #0071bc;
        }
        section {
            margin-bottom: 20px;
            padding: 10px;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
            border-radius: 5px;
            background-color: #ffffff;
            border: 1px solid #aeb0b5;
        }
        a {
            color: #0071bc;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        footer {
            background-color: #112e51;
            color: white;
            text-align: center;
            padding: 10px;
            position: relative;
            bottom: 0;
            width: 100%;
        }

        .collapsible {
            cursor: pointer;
            font-weight: bold;
            display: inline-block;
            margin-bottom: 5px;
        }

        .collapsible:hover {
            color: #0071bc;
        }

        .plus-minus {
            font-weight: bold;
            margin-left: 5px;
        }

        .collapsible h2 {
            display: inline-block;
            margin: 0; /* Optional: removes default margin */
        }
        .collapsible .plus-minus {
            margin-left: 10px; /* Add spacing between the heading and the toggle */
        }

        .content {
            padding: 5px 15px;
            display: none;
            overflow: hidden;
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            margin-bottom: 10px;
        }

        .timeline-container {
            position: relative;
            width: 95%;
            height: 450px;
            margin-top: 0px;
            margin-bottom: 0px;
            margin-left: 0px;
            margin-right: 0px;
            border-top: 2px solid #ddd;
            top: 50%;
            transform: translateY(50%); /* Centers the line vertically */
        }

        .event-box {
            position: absolute;
            width: 100px;
            padding: 5px;
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 5px;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
        }
        .event-box::after {
            content: "";
            position: absolute;
            width: 5px;
            height: 60px;
            background-color: #007bff;
        }

        .event-box.top {
            bottom: 100%;
            transform: translateY(-60px); /* Increased spacing above the timeline */
        }

        .event-box.top::after {
            top: 100%;
            left: 50%;
            transform: translateX(-50%);
        }

        .event-box.bottom {
            top: 100%;
            transform: translateY(-392px); /* Move the box closer to the timeline */
        }

        .event-box.bottom::after {
            bottom: 100%;
            left: 50%;
            transform: translateX(-50%);
        }
        .event-date {
            font-weight: bold;
            color: #007bff;
        }
        .month-label {
            position: absolute;
            top: -20px;
            font-weight: bold;
            color: #555;
        }
        .current-date-marker {
            position: absolute;
            width: 10px;
            height: 10px;
            background-color: red;
            border-radius: 50%;
            top: -4px;
            transform: translateX(-50%);
        }

        .acl-region {
            position: absolute;
            height: 60px; /* Adjust based on timeline height */
            background-color: rgba(0, 123, 255, 0.3); /* Blue with 20% opacity */
            border-top: 2px solid blue; /* Only show border at the top */
            top: -62px; /* Adjust to vertically center it on the timeline */
            width: calc(100% * ((new Date('2025-08-01') - new Date('2025-07-27')) / totalDuration)); /* Dynamic width based on date range */
            left: calc(100% * ((new Date('2025-07-27') - timelineStart) / totalDuration)); /* Position start of region */
            z-index: 1; /* Ensure it appears below the event boxes */
            text-align: center; /* Center the text */
            font-weight: bold; /* Make the text bold */
            font-size: 14px; /* Adjust text size */
            color: blue; /* Text color */
            line-height: 60px; /* Center the text vertically */
            pointer-events: none; /* Prevent user interaction */
        }


        .acl-label {
            position: absolute;
            top: -150px; /* Position the label above the ACL region */
            left: 94%;
            /* Calculate center of ACL region and offset by half the width of the label */
            text-align: center;
            font-weight: bold;
            font-size: 14px;
            color: blue;
            z-index: 2; /* Ensure it appears above the ACL region */
            white-space: nowrap; /* Prevent text from wrapping */
        }
        /* Organizers Section Styles */

        .organizers {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 30px;
        }

        .organizers div {
            text-align: center;
            width: 150px;
        }

        .organizers img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            object-fit: cover;
            margin-bottom: 10px;
            image-rendering: auto; /* Smooth rendering for high-quality images */
        }

        .references li {
            margin-bottom: 15px;
            color: #212121;
            font-size: 16px;
            line-height: 1.8;
        }

    
        @media (max-width: 768px) {
            #timeline {
                display: none;
            }
            body {
                font-size: 14px;
            }
            header, footer {
                padding: 10px;
            }
        }

        header button {
        background-color: #3399ff; /* Slightly lighter blue */
        color: white; 
        border: 2px solid white; /* White border */
        border-radius: 5px; 
        padding: 10px 20px; 
        cursor: pointer; 
        font-size: 16px; 
        font-weight: bold; 
        margin-top: 10px; 
        }

        header button:hover {
            background-color: #e6f7ff; /* Slightly lighter blue on hover */
            color: #3399ff;
            border-color: #ffffff; /* Keep the white border */
        }

        a {
            text-decoration: none; /* Remove underline from link */
        }
    
    </style>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
    <header style="display: flex; align-items: center; justify-content: center; gap: 20px; padding-left: 60px;">
        <div style="text-align: center;">
            <h1 style="margin: 20px;">ClinIQLink 2025 - LLM Lie Detector Test</h1>
            <p style="margin: 20px 0;">Evaluating Internal Model Knowledge Retrieval and Hallucinations</p>
            <a href="https://www.codabench.org/competitions/5117/" target="_blank">
                <button>View Challenge on CodaBench</button>
            </a>
            <a href="javascript:void(0);" id="expand-collapse-link">
                <button id="expand-collapse-btn">Expand All</button>
            </a>
        </div>
        <img src="Images/ClinIQLink-logo.svg" alt="ClinIQLink Logo" style="height: 100%; max-height: 160px;">
    </header>
    
    <main>
        <section>

            <div class="collapsible"><h2>Task Overview</h2><span class="plus-minus">+</span></div>
                    <div class="content"> <!--style="display: block;"-->
                        <p>
                            The objective of ClinIQLink is to <b>evaluate the ability of generative models to produce factually accurate medical information</b>. Participants must <b>submit their models to CodaBench</b>, <a href="#ref1">[1]</a>, where the organizers will test and evaluate them through a semi-automated testing process. The leaderboard will rank submissions based solely on the accuracy of the knowledge retrieved by the submitted models.
                        
                            The task is designed to assess:
                            <ul>
                                <li>
                                    <b>Knowledge Retrieval:</b> Using a novel dataset of atomic question-answer pairs, the task will measure how well generative models retrieve factually correct medical information. The evaluation will focus on fundamental medical concepts (aimed at the knowledge level of a General Practicioner (GP)), such as procedures, conditions, drugs, and diagnostics. Scoring will be based on precision, with full points awarded for exact or semantically equivalent answers. Incorrect or factually inaccurate responses will result in negative scores.
                                </li>
                            </ul>
                        
                            <b>Post-hoc Analysis:</b> While participants will be evaluated solely on their ability to provide factually accurate medical knowledge, the coordinators will conduct post-hoc analysis of the submitted models to identify and categorize hallucinations in the responses. This analysis will focus on understanding the origins of hallucinations, including:
                            <ul>
                                <li><b>Intrinsic:</b> Hallucinations caused by the model's internal representations.</li>
                                <li><b>Extrinsic:</b> Hallucinations arising due to missing or incorrect external information.</li>
                                <li><b>Other:</b> Hallucinations from unknown or hybrid causes.</li>
                            </ul>
                            The post-hoc findings will provide insights into the limitations of the models but will not impact the participants' scores.
                        
                            <b>Participation Requirements:</b> To be considered for acceptance, participants must submit a <b>short paper</b> outlining their novel model or method. The short paper must present innovative approaches or significant contributions to medical knowledge retrieval.  The shared task will utilize a <b>novel medical QA dataset that will not be made publicly available</b> to ensure the integrity of the evaluation process.
                        </p>
                        
                    </div>
            
        </section>

        <section>

            <div class="collapsible"><h2>Datasets</h2> <span class="plus-minus">+</span></div>
            <div class="content"><!--style="display: block;"-->
                <p>
                    The dataset provided for this task is a <b>novel collection of factual, atomic question-answer pairs</b> grounded in the medical domain, designed to align with the knowledge level of a General Practitioner (GP) Medical Doctor. The dataset is generated with input from medical experts, and each question is supported by source documentation from medical textbooks to ensure accuracy.
                
                    The dataset covers core medical concepts, including procedures, conditions, drugs, and diagnostics, and consists of the following five modalities:
                </p>
                
                <div class="collapsible-container">
            
                    <div class="collapsible">True/False <span class="plus-minus">+</span></div>
                    <div class="content">
                        <p><strong>Question:</strong><br><em>"Antibiotics can treat viral infections."</em></p>
                        <p><strong>Answer:</strong><br><em>"False."</em></p>
                    </div>

                    <br>
            
                    <div class="collapsible">Multiple Choice <span class="plus-minus">+</span></div>
                    <div class="content">
                        <p><strong>Question:</strong><br><em>"Which of the following is not a symptom of diabetes?"</em></p>
                        <p><strong>Options:</strong></p>
                        <ul>
                            <li>A. Increased thirst</li>
                            <li>B. Frequent urination</li>
                            <li>C. Blurred vision</li>
                            <li>D. High fever</li>
                        </ul>
                        <p><strong>Correct Answer:</strong><br><em>"D. High fever"</em></p>
                    </div>

                    <br>

                    <div class="collapsible">List <span class="plus-minus">+</span></div>
                    <div class="content">
                        <p><strong>Question:</strong><br><em>"List the four chambers of the human heart."</em></p>
                        <p><strong>Options:</strong></p>
                        <ul>
                            <li>A. right atrium</li>
                            <li>B. top atrium</li>
                            <li>C. right ventricle</li>
                            <li>D. bottom ventricle</li>
                            <li>E. left atrium</li>
                            <li>F. left ventricle</li>
                        </ul>
                        <p><strong>Answer:</strong><br><em>"A. right atrium, C. right ventricle, E. left atrium, F. left ventricle."</em></p>
                    </div>

                    <br>

                    <div class="collapsible">Short Answer <span class="plus-minus">+</span></div>
                    <div class="content">
                        <p><strong>Question:</strong><br><em>"What is a Laparoscopy?"</em></p>
                        <p><strong>Answer:</strong><br><em>"A minimally invasive surgical procedure that uses a camera to view the inside of the abdomen."</em></p>
                    </div>

                    <br>

                    <div class="collapsible"><strong>Multi-Hop Knowledge-Retrieval Answers</strong><span class="plus-minus">+</span></div>
                    <div class="content">
                        <p><strong>Question:</strong><br><em>"A patient presents with fatigue and shortness of breath. Lab results show low hemoglobin levels and high mean corpuscular volume (MCV). Based on these findings, what condition might the patient have, and what deficiency could be contributing to it?"</em></p>
                        <p><strong>Answer:</strong><br><em>"The patient might have megaloblastic anemia caused by a vitamin B12 deficiency."</em></p>
                        <p><strong>Reasoning:</strong><br><em>"Step 1: Fatigue and shortness of breath are symptoms of anemia.<br>Step 2: High MCV indicates macrocytic anemia.<br>Step 3: Macrocytic anemia is commonly caused by vitamin B12 or folate deficiency.<br>Step 4: Conclude megaloblastic anemia due to likely vitamin B12 deficiency."</em></p>
                    </div>

                </div>
            </div>

            
            
        </section>

        <section>

            <div class="collapsible"><h2>Evaluation Metrics</h2><span class="plus-minus">+</span></div>
                <div class="content"> <!--style="display: block;"-->
                    <p>Our evaluation framework is organized into two primary categories: (1) Closed-ended questions (True/False, lists, and multiple choice) and (2) Open-ended questions (short answer and Multi-Hop Knowledge-Retrieval Answers).</p>
                    <div class="collapsible-container">

                        <div class="collapsible">Closed-ended questions<span class="plus-minus">+</span></div>
                        <div class="content">
                
                            <p>
                                Closed-ended questions include True/False, list, and multiple-choice formats. These are evaluated using the <strong>F1 score</strong> <a href="#ref2">[2]</a> and a penalty system for factually inaccurate answers.
                            </p>
                
                            <h3>F1 Score Definition</h3>
                            <p>The F1 score is the harmonic mean of <strong>precision</strong> and <strong>recall</strong>.</p>
                            <ul>
                                <li><strong>Precision:</strong> The proportion of correctly predicted answers out of all predicted answers.
                                    \[
                                    \text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
                                    \]
                                </li>
                                <li><strong>Recall:</strong> The proportion of correctly predicted answers out of all actual correct answers.
                                    \[
                                    \text{Recall} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
                                    \]
                                </li>
                                <li><strong>F1 Score:</strong> 
                                    \[
                                    \text{F1 Score} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
                                    \]
                                </li>
                            </ul>
                            
                            <br>

                            <div class="collapsible">True/False Questions<span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>
                                    True/False questions are evaluated by comparing the model’s response to the expected answer, with scoring based on precision, recall, and F1.
                                </p>
                                <p><strong>Example:</strong></p>
                                <ul>
                                    <li><strong>Question:</strong> "Antibiotics can treat viral infections?"</li>
                                    <li><strong>Expected Answer:</strong> "False"</li>
                                    <li><strong>Model Answer:</strong> "True"</li>
                                    <li><strong>Result:</strong> TP = 0, FP = 0, FN = 1</li>
                                </ul>
                            </div>

                            <br>
                
                            <div class="collapsible">List Questions<span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>
                                    List questions are evaluated by comparing individual items in the model’s response to the expected list. Each correctly identified item is considered a True Positive (TP), while missing items count as False Negatives (FN), and incorrect items as False Positives (FP).
                                </p>
                                <p><strong>Example:</strong></p>
                                <ul>
                                    <li><strong>Question:</strong> "List the four chambers of the human heart."</li>
                                    <li><strong>Options:</strong></li>
                                    <ul>
                                        <li>A. right atrium</li>
                                        <li>B. top atrium</li>
                                        <li>C. right ventricle</li>
                                        <li>D. bottom ventricle</li>
                                        <li>E. left atrium</li>
                                        <li>F. left ventricle</li>
                                    </ul>
                                    <li><strong>Expected Answer:</strong> "A. right atrium, C. right ventricle, E. left atrium, F.left ventricle."</li>
                                    <li><strong>Model Answer:</strong> "A. Right atrium, B. top atrium, C. right ventricle, E. left atrium."</li>
                                    <li><strong>Result:</strong> TP = 3, FP = 1, FN = 1</li>
                                </ul>
                            </div>

                            <br>
                
                            <div class="collapsible">Multiple Choice Questions<span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>
                                    Multiple-choice questions are scored by checking if the selected option matches the correct answer. A correct selection is a True Positive (TP), an incorrect selection counts as a False Positive (FP), and a failure to answer or an irrelevant response counts as a False Negative (FN).
                                </p>
                                <p><strong>Example:</strong></p>
                                <ul>
                                    <li><strong>Question:</strong> "Which of the following is not a symptom of diabetes?"</li>
                                    <li><strong>Options:</strong></li>
                                    <ul>
                                        <li>A. Increased thirst</li>
                                        <li>B. Frequent urination</li>
                                        <li>C. Blurred vision</li>
                                        <li>D. High fever</li>
                                    </ul>
                                    <li><strong>Expected Answer:</strong> "D. High fever"</li>
                                    <li><strong>Model Answer:</strong> "B. Frequent urination"</li>
                                    <li><strong>Result:</strong> TP = 0, FP = 1, FN = 0</li>
                                </ul>
                            </div>

                            <br>
                
                        </div>
                
                        <br>
                
                        <div class="collapsible">Open-ended questions<span class="plus-minus">+</span></div>
                        <div class="content">
                            <p>
                                Open-ended questions scoring is divided into two key levels of semantic similarity:

                                <div class="collapsible">Exact Match (Full Points)<span class="plus-minus">+</span></div>
                                <div class="content">
                                    <ul>
                                        <li>Full points will be awarded if the model provides an exact match to the known ground truth answer.</li>
                                        <li><strong>Example:</strong>
                                            <ul>
                                                <li><strong>Question:</strong> "What is Laparoscopy?"</li>
                                                <li><strong>Expected Answer:</strong> "A minimally invasive surgical procedure using a camera."</li>
                                                <li><strong>Model Answer:</strong> "A minimally invasive surgical procedure using a camera."</li>
                                                <li><strong>Scoring:</strong> If the model returns exactly this answer, full points are awarded.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>

                            <br>

                            <div class="collapsible">Full to Partial Semantic Match (Full to Partial Points)<span class="plus-minus">+</span></div>
                                <div class="content">
                                    <ul>
                                        <li> Full points will also be awarded if the model returns an answer that is semantically equivalent to the ground truth, even if phrased differently (e.g., using synonyms). BLEU <a href="#ref3">[3]</a>, ROUGE <a href="#ref4">[4]</a> and METEOR <a href="#ref5">[5]</a> scores will be calculated for all generated-reference QA response pairs and, <b>in cases where automated metrics may yield uncertain or inconsistent scores, human annotators will be employed in conjuction with those scores along with the semantic similarity scores described below to review and normalize the evaluation. </b></li>
                                        
                                        <li><strong>Scoring for Semantic Answer Matching:</strong>
                                            <p>
                                                The semantic similarity score combines word-level, sentence-level, and paragraph-level similarity. Each similarity level is calculated using embeddings and cosine similarity, and the final score is computed as a weighted sum of these individual scores:
                                            </p>
                                            <div style="background-color: #f9f9f9; padding: 10px; border: 1px solid #ddd;">
                                                \[
                                                \text{Semantic Match Score} = w_{word} \cdot \text{CosineSim}_{word} + w_{sentence} \cdot \text{CosineSim}_{sentence} + w_{paragraph} \cdot \text{CosineSim}_{paragraph}
                                                \]
                                                Where:
                                                <ul>
                                                    <li>
                                                        \(w_{word}, w_{sentence}, w_{paragraph}\) are weights that sum to 1.
                                                    </li>
                                                    <li>
                                                        \(\text{CosineSim}_{word}\): Mean pooling of word embeddings obtained from the model output using:
                                                        \[
                                                        E_{\text{sentence}} = \frac{\sum_{i=1}^{T} E_{\text{token}_i} \cdot \text{Mask}_i}{\sum_{i=1}^{T} \text{Mask}_i}
                                                        \]
                                                        Where:
                                                        <ul>
                                                            <li>\(E_{\text{token}_i}\) is the embedding for the \(i\)-th token.</li>
                                                            <li>\(\text{Mask}_i\) is the attention mask for the \(i\)-th token to exclude padding tokens.</li>
                                                            <li>\(T\) is the total number of tokens.</li>
                                                        </ul>
                                                    </li>
                                                    <br>
                                                    <br>
                                                    <li>
                                                        \(\text{CosineSim}_{sentence}\): Cosine similarity between corresponding sentence embeddings \(E_x\) and \(E_y\), calculated as:
                                                        \[
                                                        \text{CosineSimilarity}(E_x, E_y) = \frac{E_x \cdot E_y}{\|E_x\| \|E_y\|}
                                                        \]
                                                        Where:
                                                        <ul>
                                                            <li>\(E_x \cdot E_y\) is the dot product of the embeddings.</li>
                                                            <li>\(\|E_x\|\) and \(\|E_y\|\) are the magnitudes of the embeddings.</li>
                                                        </ul>
                                                        <p>Once cosine similarity is computed for each corresponding sentence pair in the generated and reference answers, the aggregated sentence-level similarity is obtained by averaging all pairwise similarities:</p>
                                                        \[
                                                        \text{CosineSim}_{\text{sentence}} = \frac{1}{N} \sum_{i=1}^{N} \text{CosineSimilarity}(E_{x_i}, E_{y_i})
                                                        \]
                                                        Where \(N\) is the number of sentences in the generated and reference answers.
                                                    </li>
                                                    <br>
                                                    <br>
                                                    <li>
                                                        \(\text{CosineSim}_{paragraph}\): Cosine similarity between the full answer embeddings, calculated as:
                                                        \[
                                                        \text{CosineSimilarity}(E_{\text{paragraph, gen}}, E_{\text{paragraph, ref}}) = \frac{E_{\text{paragraph, gen}} \cdot E_{\text{paragraph, ref}}}{\|E_{\text{paragraph, gen}}\| \|E_{\text{paragraph, ref}}\|}
                                                        \]
                                                        Where:
                                                        <ul>
                                                            <li>\(E_{\text{paragraph, gen}}\) and \(E_{\text{paragraph, ref}}\) are the paragraph embeddings obtained by mean pooling over all token embeddings in the generated and reference answers, respectively:</li>
                                                            \[
                                                            E_{\text{paragraph}} = \frac{\sum_{i=1}^{T} E_{\text{token}_i} \cdot \text{Mask}_i}{\sum_{i=1}^{T} \text{Mask}_i}
                                                            \]
                                                            <li>\(T\) is the total number of tokens in the paragraph.</li>
                                                        </ul>
                                                    </li>
                                                </ul>
                                            </div>

                                            <p>
                                                The word-level score will be calculated using a method similar to BERTScore <a href="#ref6">[6]</a>, which evaluates text generation using contextual embeddings. 
                                                The sentence-level and paragraph-level scores will be computed in a manner inspired by SemScore <a href="#ref7">[7]</a>, which leverages semantic textual similarity for evaluating instruction-tuned large language models.
                                            </p>
                                
                                            <p>The final score is determined as follows:</p>
                                            <ul>
                                                <li><strong>Full Match:</strong> If the semantic match score exceeds the semantic similarity threshold (0.9), full points are awarded.</li>
                                                <li><strong>Partial Match:</strong> If the semantic match score lies between two thresholds of no semantic similarity and full semantic similarity (0.4 to 0.9), partial points are awarded using a linear interpolation:
                                                    \[
                                                    \text{Partial Points} = \frac{\text{Semantic Match Score} - \text{Lower Threshold}}{\text{Full Threshold} - \text{Lower Threshold}} \times \text{Max Points}
                                                    \]
                                                </li>
                                            </ul>
                                        </li>
                                        

                                        <li><strong>Full Match Example:</strong>
                                            <ul>
                                                <li><strong>Question:</strong> "What is Laparoscopy?"</li>
                                                <li><strong>Expected Answer:</strong> "A minimally invasive surgical procedure using a camera."</li>
                                                <li><strong>Model Answer:</strong> "A minimally invasive surgery with a camera."</li>
                                                <li><strong>Scoring:</strong> Full Points (1).</li>
                                            </ul>
                                        </li>

                                        <li><strong>Partial Match Example:</strong>
                                            <ul>
                                                <li><strong>Question:</strong> "What is Laparoscopy?"</li>
                                                <li><strong>Expected Answer:</strong> "A minimally invasive surgical procedure using a camera."</li>
                                                <li><strong>Model Answer:</strong> "A procedure using a camera."</li>
                                                <li><strong>Scoring:</strong> Partial Points (less than 1).</li> <!--TODO - update to have the semantic scoring number -->
                                            </ul>
                                        </li>
                                    </ul>
                                </div>

                            <br>
                            </p>

                        </div>


                    </div>
                
                </div>
            

        

        </section>

        <section>
            <div class="collapsible"><h2>Submission, Rules and Participation Requirements</h2><span class="plus-minus">+</span></div>
                    <div class="content"> <!--style="display: block;"-->
                        <div class="collapsible-container">
                            <div class="collapsible">Submission Format<span class="plus-minus">+</span></div>
                            <div class="content">

                                All participants will be invited to submit a paper describing their solution to be included in the <a href="https://aclanthology.org/venues/bionlp/" style="color: #0071bc; text-decoration: none;">Proceedings of the 24rd Workshop on Biomedical Natural Language Processing (BioNLP) at ACL 2025</a>

                                <ul>
                                    <li>
                                        Participants are required to <b>submit their models to CodaBench</b>, <a href="#ref1">[1]</a> where the organizers will test and evaluate the submissions through a semi-automated process. Participants are not required to make their submissions public; however, doing so is highly encouraged to promote transparency and reproducibility.
                                    </li>
                                    <li>
                                        Submissions <b>must</b> include:
                                        <ul>
                                            <li>The model uploaded to CodaBench <a href="#ref1">[1]</a> for evaluation.</li>
                                            <li>A detailed description of the system approach, including the methods, training data, configurations, and techniques used to develop the <b> novel model/method</b> in the form of a <b>short paper </b>.</li>
                                        </ul>
                                    </li>
                                    <li>
                                        Optional: 
                                        <ul>
                                            <li> Participants can make their models and approaches public to foster collaboration and transparency within the community.</li>
                                        </ul>
                                    </li>
                                </ul>
                                
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Model Transparency<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>Participants are required to provide clear documentation about:
                                        <ul>
                                            <li>The methods and techniques used, including any training data, preprocessing steps, fine-tuning strategies, and specific algorithms.</li>
                                            <li>Any prompts, prompt engineering, or inference strategies employed.</li>
                                        </ul>
                                    </li>
                                    <li>This information will be used to ensure transparency and promote openness within the competition but will not affect leaderboard rankings.</li>
                                </ul>
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Training and Data Use<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>Participants are free to use any methods, models, or data sources to achieve the best results.</li>
                                    <li>There are no restrictions on the choice of training data, models, or tools. Participants can combine datasets, use proprietary or private data, and leverage any strategies or algorithms to maximize performance.</li>
                                    <li><strong>Responsibility:</strong> Participants must ensure their approach complies with all applicable legal and ethical standards regarding data usage and methodology.</li>
                                </ul>
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Submission Limits<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>Each participant or team may submit a maximum of three times for consideration on the leaderboard.</li>
                                    <li>Each submission must reflect a distinct system or approach.</li>
                                </ul>
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Leaderboard Ranking<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>Submissions will be ranked on the official leaderboard based solely on the evaluation metrics generated by the provided script.</li>
                                    <li>The leaderboard will reflect scores based on exact matches, semantic matches, partial matches, and penalize factually incorrect responses.</li>
                                    <li>No distinction will be made between submissions using public or proprietary methods in the rankings. All valid submissions will appear on the leaderboard based on their scores.</li>
                                </ul>
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Use of ClinIQLink Dataset<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>The fully annotated ClinIQLink dataset will <b> NOT</b> be released publicly for participants to use.</li>
                                </ul>
                            </div>
            
                            <br>
                        
                            <div class="collapsible">Evaluation Requirements<span class="plus-minus">+</span></div>
                            <div class="content">
                                <ul>
                                    <li>Participants are required to use the official evaluation script provided by the organizers to generate their metrics.</li>
                                    <li>Submissions must strictly adhere to the output format specified by the script to ensure comparability across all entries.</li>
                                </ul>
                            </div>
                        
                        </div>
                    </div>


            
            
        </section>

        <section>
            <div class="collapsible"><h2>Timeline</h2><span class="plus-minus">+</span></div>
                    <div class="content"> 
                        <p>Timeline for ClinIQLink at BioNLP Workshop at ACL 2025:</p>
                        <div class="timeline-container" id="timeline">
                            <!-- Month Labels -->
                            <div class="month-label" style="left: 0%;" id="label-Jan">Jan</div>
                            <div class="month-label" style="left: 0%;" id="label-Feb">Feb</div>
                            <div class="month-label" style="left: 0%;" id="label-Mar">Mar</div>
                            <div class="month-label" style="left: 0%;" id="label-Apr">Apr</div>
                            <div class="month-label" style="left: 0%;" id="label-May">May</div>
                            <div class="month-label" style="left: 0%;" id="label-Jun">Jun</div>
                            <div class="month-label" style="left: 0%;" id="label-Jul">Jul</div>
                            <div class="month-label" style="left: 0%;" id="label-Aug">Aug</div>
                        
                            <!-- Events -->
                            <div class="event-box top" id="event-first_call">
                                <div class="event-date">January 21, 2025</div>
                                First Call for Participation
                            </div>
                            <div class="event-box top" id="event-release">
                                <div class="event-date">February 20, 2025</div>
                                Release of Sample Submission Dataset (Github)
                            </div>
                            <div class="event-box bottom" id="event-release-Coda">
                                <div class="event-date">March 01, 2025</div>
                                Release of Testing Dataset & Framework (CodaBench)
                            </div>
                            <div class="event-box top" id="event-system_sub">
                                <div class="event-date"> April 15, 2025</div>
                                System Submission Deadline
                            </div>
                            <div class="event-box top" id="event-feedback">
                                <div class="event-date"> April 25, 2025</div>
                                Results Feedback Provided
                            </div>
                            <div class="event-box bottom" id="event-paper_sub">
                                <div class="event-date">May 05, 2025</div>
                                Prelim Papers Submission Deadline
                            </div>
                            <div class="event-box top" id="event-paper_sub_final">
                                <div class="event-date">May 15, 2025</div>
                                Final Papers Submission Deadline
                            </div>
                            <div class="event-box bottom" id="event-notif">
                                <div class="event-date">May 10, 2025</div>
                                Notification of Acceptance
                            </div>
                            <div class="event-box top" id="event-cam">
                                <div class="event-date">May 20, 2025</div>
                                Camera Ready Papers Due
                            </div>
                            <div class="event-box top" id="event-record">
                                <div class="event-date">July 7, 2025</div>
                                Pre-recorded Video Due
                            </div>
                            <div class="event-box bottom" id="event-BioNLP">
                                <div class="event-date">July 31, 2025</div>
                                BioNLP Workshop Date (at ACL 2025)
                            </div>

                            <!-- Label for ACL region -->
                            <div class="acl-label">ACL 2025 <br> July 27th 2025 - <br> August 01 2025</div>
                            <!-- Semi-transparent region for ACL Conference -->
                            <div class="acl-region" id="acl-region"></div>

                            <!-- Current Date Marker -->
                            <div class="current-date-marker" id="current-date"></div>
                        </div>

                        <ol style="font-size: 16px; line-height: 1.8; margin-left: 20px;">
                            <li id="event-first_call">
                                <strong>January 21, 2025:</strong> First Call for Participation
                            </li>
                            <li id="event-release">
                                <strong>February 20, 2025:</strong> Release of Sample Submission Dataset (Github)
                            </li>
                            <li id="event-release-Coda">
                                <strong>March 01, 2025:</strong> Release of Testing Dataset & Framework (CodaBench)
                            </li>
                            <li id="event-system_sub">
                                <strong>April 15, 2025:</strong> System Submission Deadline
                            </li>
                            <li id="event-feedback">
                                <strong>April 25, 2025:</strong> Results Feedback Provided
                            </li>
                            <li id="event-paper_sub">
                                <strong>May 05, 2025:</strong> Prelim Papers Submission Deadline
                            </li>
                            <li id="event-paper_sub_final">
                                <strong>May 15, 2025:</strong> Final Papers Submission Deadline
                            </li>
                            <li id="event-notif">
                                <strong>May 10, 2025:</strong> Notification of Acceptance
                            </li>
                            <li id="event-cam">
                                <strong>May 20, 2025:</strong> Camera Ready Papers Due
                            </li>
                            <li id="event-record">
                                <strong>July 07, 2025:</strong> Pre-recorded Video Due
                            </li>
                            <li id="event-BioNLP">
                                <strong>July 31, 2025:</strong> BioNLP Workshop Date (at ACL 2025)
                            </li>
                        </ol>
                        
                        <!-- ACL Conference Region -->
                        <p class="acl-label" style="font-weight: bold; margin-top: 20px;">
                            ACL 2025: July 27th 2025 - August 01 2025
                        </p>
    
                        <b>All deadlines are 11:59 PM ("Anywhere on Earth")</b>
                </div>
                
        </section>

        <section>
            <div class="collapsible"><h2>Research Directions</h2><span class="plus-minus">+</span></div>
                    <div class="content">  <!--style="display: block;"-->
                        <div class="collapsible"><strong>LLM Hallucination Detection</strong><span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>See the following survey papers for ideas on research directions:</p>
                                <ul>
                                    <li>
                                        <strong>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</strong><br>
                                        <a href="https://arxiv.org/abs/2309.01219">https://arxiv.org/abs/2309.01219</a>
                                    </li>
                                    <li>
                                        <strong>Survey of Hallucination in Natural Language Generation</strong><br>
                                        <a href="https://dl.acm.org/doi/full/10.1145/3571730?casa_token=4qvgx-DzJjkAAAAA%3AOwIpns22iVZpou0ArNv0kCCt6_UXCKxivtJtN7V6YdMkX0VfeuNUG6aKi0mmdCblaZPPj75X_-46oQ">https://dl.acm.org/doi/full/10.1145/3571730</a>
                                    </li>
                                    <li>
                                        <strong>A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions</strong><br>
                                        <a href="https://arxiv.org/abs/2311.05232">https://arxiv.org/abs/2311.05232</a>
                                    </li>
                                    <li>
                                        <strong>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models</strong><br>
                                        <a href="https://arxiv.org/abs/2401.01313">https://arxiv.org/abs/2401.01313</a>
                                    </li>
                                    <li>
                                        <strong>Hallucination of Multimodal Large Language Models: A Survey</strong><br>
                                        <a href="https://arxiv.org/abs/2404.18930">https://arxiv.org/abs/2404.18930</a>
                                    </li>
                                    <li>
                                        <strong>A Survey of Hallucination in Large Foundation Models</strong><br>
                                        <a href="https://arxiv.org/abs/2309.05922">https://arxiv.org/abs/2309.05922</a>
                                    </li>
                                    <li>
                                        <strong>A Survey on Hallucination in Large Vision-Language Models</strong><br>
                                        <a href="https://arxiv.org/abs/2402.00253">https://arxiv.org/abs/2402.00253</a>
                                    </li>
                                    <li>
                                        <strong>TruthfulQA: Measuring How Models Mimic Human Falsehoods</strong><br>
                                        <a href="https://arxiv.org/pdf/2109.07958">https://arxiv.org/pdf/2109.07958</a>
                                    </li>
                                    <li>
                                        <strong>TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space</strong><br>
                                        <a href="https://arxiv.org/pdf/2402.17811">https://arxiv.org/pdf/2402.17811</a>
                                    </li>
                                    <li>
                                        <strong>The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models</strong><br>
                                        <a href="https://arxiv.org/pdf/2401.03205">https://arxiv.org/pdf/2401.03205</a>
                                    </li>
                                </ul>
                            
                            </div>
                        <br>
                        <div class="collapsible"><strong>Retrieval Augmented Generation</strong><span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>Retrieval-Augmented Generation (RAG) combines large language models with external retrieval mechanisms to enhance factual accuracy and mitigate hallucinations. The following papers explore various aspects of RAG:</p>
                                <ul>
                                    <li>
                                        <strong>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</strong><br>
                                        <a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a><br>
                                    </li>
                                    <li>
                                        <strong>Retrieval-Augmented Generation for Large Language Models: A Survey</strong><br>
                                        <a href="https://arxiv.org/abs/2312.10997">https://arxiv.org/abs/2312.10997</a><br>
                                    </li>
                                    <li>
                                        <strong>Incorporating Retrieval into Generation Models</strong><br>
                                        <a href="https://arxiv.org/abs/2406.13249">https://arxiv.org/abs/2406.13249</a><br>
                                    </li>
                                    <li>
                                        <strong>A Comprehensive Survey of Retrieval-Augmented Generation</strong><br>
                                        <a href="https://arxiv.org/abs/2410.12837">https://arxiv.org/abs/2410.12837</a><br>
                                    </li>
                                </ul>
                            </div>
                        <br>
                        <div class="collapsible"><strong>Medical QA Datasets</strong><span class="plus-minus">+</span></div>
                        <div class="content">
                            <p>Key datasets commonly used for Medical Question Answering research involving Large Language Models include:</p>
                            <ul>
                                <li>
                                    <strong>MultiMedQA</strong><br>
                                    <a href="https://arxiv.org/abs/2212.13138">https://arxiv.org/abs/2212.13138</a>
                                </li>
                                <li>
                                    <strong>MedQA</strong><br>
                                    <a href="https://arxiv.org/abs/2009.13081">https://arxiv.org/abs/2009.13081</a>
                                </li>
                                <li>
                                    <strong>MedMCQA</strong><br>
                                    <a href="https://arxiv.org/abs/2203.14371">https://arxiv.org/abs/2203.14371</a>
                                </li>
                                <li>
                                    <strong>PubMedQA</strong><br>
                                    <a href="https://pubmedqa.github.io/">https://pubmedqa.github.io/</a>
                                </li>
                                <li>
                                    <strong>BioASQ</strong><br>
                                    <a href="https://www.bioasq.org/">https://www.bioasq.org/</a>
                                </li>
                            </ul>
                        </div>
                        <br>
                        <div class="collapsible"><strong>Medical QA Models</strong><span class="plus-minus">+</span></div>
                        <div class="content">
                            <ul>
                                <li>
                                    <strong>Med-PaLM 2</strong><br>
                                    <a href="https://arxiv.org/abs/2305.09617" target="_blank">https://arxiv.org/abs/2305.09617</a>
                                </li>
                                <li>
                                    <strong>PMC-LLaMA</strong><br>
                                    <a href="https://arxiv.org/abs/2304.14454" target="_blank">https://arxiv.org/abs/2304.14454</a>
                                </li>
                                <li>
                                    <strong>Integrating UMLS Knowledge into LLMs</strong><br>
                                    <a href="https://arxiv.org/abs/2310.02778" target="_blank">https://arxiv.org/abs/2310.02778</a>
                                </li>
                                <li>
                                    <strong>Exploring the Landscape of LLMs in Medical QA</strong><br>
                                    <a href="https://arxiv.org/abs/2310.07225" target="_blank">https://arxiv.org/abs/2310.07225</a>
                                </li>
                                <li>
                                    <strong>Hugging Face Medical QA Leaderboard</strong><br>
                                    <a href="https://huggingface.co/blog/leaderboard-medicalllm" target="_blank">https://huggingface.co/blog/leaderboard-medicalllm</a>
                                </li>
                            </ul>
                        </div>
                        

                    </div>
            
        </section>

        <section>
            <div class="collapsible"><h2>Support and FAQs</h2><span class="plus-minus">+</span></div>
                    <div class="content"> <!--style="display: block;"-->
                        <div class="collapsible"><strong>Support Contact</strong><span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>For all questions involving the shared task, please contact <strong>Brandon Colelough</strong></p>
                                <p>Email: <a href="mailto:brandon.colelough@nih.giv">brandon.colelough@nih.giv</a></p>
                            </div>

                        <br>

                        <div class="collapsible"><strong>FAQs</strong><span class="plus-minus">+</span></div>
                            <div class="content">
                                <p>Coming soon</p>
                            </div>
                    </div>
        </section>

        <section>
            <div class="collapsible"><h2>Result Reporting and Post-task Analysis</h2> <span class="plus-minus">+</span></div>
                    <div class="content"> <!--style="display: block;"-->
                        <p>Coming Soon</p>
                    </div>
        </section>
        <section>
            <div class="collapsible"><h2>Organizers</h2><span class="plus-minus">+</span></div>
                <div class="content"> 
                    <div style="display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px; margin-top: 20px;">
                        <!-- Organizer 1 -->
                        <div style="text-align: center;">
                            <img src="Images/Brandon-headshot-no_background.png" alt="Brandon Colelough" style="width: 200px; height: 200px; border-radius: 50%; object-fit: cover;">
                            <h3 style="margin: 10px 0;">Brandon Colelough</h3>
                            <a href="https://brandoncolelough.com/" style="color: #0071bc; text-decoration: none;">Website</a>
                        </div>
                        <!-- Organizer 1 -->
                        <div style="text-align: center;">
                            <img src="Images/DinaDemnerFushman_headshot.jpg" alt="Dina Demner-Fushman" style="width: 200px; height: 200px; border-radius: 50%; object-fit: cover;">
                            <h3 style="margin: 10px 0;">Dina Demner-Fushman</h3>
                            <a href="https://www.nlm.nih.gov/research/researchstaff/DemnerFushmanDina.html" style="color: #0071bc; text-decoration: none;">Website</a>
                        </div>
                        <!-- Organizer 2 -->
                        <div style="text-align: center;">
                            <div style="width: 150px; height: 150px; border-radius: 50%; background-color: #d3d3d3; display: flex; align-items: center; justify-content: center;">
                                <span style="font-size: 80px; color: white;">N/A</span>
                            </div>
                            <h3 style="margin: 10px 0;">Davis Bartels</h3>
                            <span style="color: #a9a9a9;">No Website</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <section>
            <div class="collapsible" id="references-section">
                <h2>References</h2><span class="plus-minus">+</span>
            </div>
            <div class="content" id="references-content" style="display: none;"> 
                <ul style="font-size: 16px; line-height: 1.8; margin-left: 20px; list-style: none; padding: 0;">
                    <li id="ref1">[1] Z. Xu et al., “Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform,” Patterns, vol. 3, no. 7, pp. 100543-100543, Jul. 2022, doi: <a href="https://doi.org/10.1016/j.patter.2022.100543" style="color: #0071bc; text-decoration: none;">https://doi.org/10.1016/j.patter.2022.100543</a>.</li>
                    <li id="ref2">[2] C. J. van Rijsbergen. "Information Retrieval." Butterworth-Heinemann, 1979.</li>
                    <li id="ref3">[3] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</li>
                    <li id="ref4">[4] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.</li>
                    <li id="ref5">[5] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan. Association for Computational Linguistics.</li>
                    <li id="ref6">[6] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “BERTScore: Evaluating Text Generation with BERT,” arXiv.org, 2019. <a href="https://arxiv.org/abs/1904.09675" style="color: #0071bc; text-decoration: none;">https://arxiv.org/abs/1904.09675</a>.</li>
                    <li id="ref7">[7] Ansar Aynetdinov and Alan Akbik. 2024. SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. arXiv. doi: <a href="https://doi.org/10.48550/arxiv.2401.17072" style="color: #0071bc; text-decoration: none;">https://doi.org/10.48550/arxiv.2401.17072</a>. URL: <a href="https://arxiv.org/abs/2401.17072" style="color: #0071bc; text-decoration: none;">https://arxiv.org/abs/2401.17072</a>.</li>
                </ul>
            </div>
        </section>
        
        
        
        
        

    </main>
    <footer>
        <p>&copy; 2025 NIH NLM - ClinIQLink - LLM Lie Detector Task @ BioNLP @ ACL. All rights reserved.</p>
    </footer>
</body>

<script>

    // Function to detect if the platform is mobile
function isMobilePlatform() {
    return /Mobi|Android|iPhone|iPad|iPod/i.test(navigator.userAgent);
}

// Hide the timeline-container for mobile platforms
function handleTimelineVisibility() {
    const timelineContainer = document.getElementById('timeline');
    if (isMobilePlatform() && timelineContainer) {
        timelineContainer.style.display = 'none'; // Hide the timeline
    }
}

// Execute the function on page load
window.onload = handleTimelineVisibility;


    // Define start and end dates for the timeline
    const timelineStart = new Date('2024-12-31');
    const timelineEnd = new Date('2025-08-05');
    const totalDurationInDays = (timelineEnd - timelineStart) / (1000 * 60 * 60 * 24); // Total duration in days

    // Define month data with IDs and number of days in each month
    const months = [
        { id: 'label-Jan', startDate: new Date('2025-01-01'), days: 31 },
        { id: 'label-Feb', startDate: new Date('2025-02-01'), days: 28 }, // Update to 29 for leap year if necessary
        { id: 'label-Mar', startDate: new Date('2025-03-01'), days: 31 },
        { id: 'label-Apr', startDate: new Date('2025-04-01'), days: 30 },
        { id: 'label-May', startDate: new Date('2025-05-01'), days: 31 },
        { id: 'label-Jun', startDate: new Date('2025-06-01'), days: 30 },
        { id: 'label-Jul', startDate: new Date('2025-07-01'), days: 31 },
        { id: 'label-Aug', startDate: new Date('2025-08-01'), days: 5 } // Partial month ending on Aug 15
    ];

    // Function to dynamically position month labels on the timeline
    let cumulativeDays = 0;
    months.forEach((month) => {
        const positionPercentage = (cumulativeDays / totalDurationInDays) * 100;
        const monthElement = document.getElementById(month.id);
        if (monthElement) {
            monthElement.style.left = `${positionPercentage}%`;
        }
        cumulativeDays += month.days;
    });

    // Define event data with IDs and dates
    const events = [
        { id: 'event-first_call', date: new Date('2025-01-21') },
        { id: 'event-release', date: new Date('2025-02-20') },
        { id: 'event-release-Coda', date: new Date('2025-03-01') },
        { id: 'event-system_sub', date: new Date('2025-04-15') },
        { id: 'event-feedback', date: new Date('2025-04-25') },
        { id: 'event-paper_sub', date: new Date('2025-05-05') },
        { id: 'event-paper_sub_final', date: new Date('2025-05-15') },
        { id: 'event-notif', date: new Date('2025-05-10') },
        { id: 'event-cam', date: new Date('2025-05-20') },
        { id: 'event-record', date: new Date('2025-07-07') },
        { id: 'event-BioNLP', date: new Date('2025-07-31') }
    ];

   // Function to dynamically position events on the timeline (center alignment)
    events.forEach((event) => {
        const elapsedDays = (event.date - timelineStart) / (1000 * 60 * 60 * 24); // Days elapsed since start
        const positionPercentage = (elapsedDays / totalDurationInDays) * 100;
        const eventElement = document.getElementById(event.id);

        if (eventElement) {
            const eventBoxWidth = 100; // eventElement.offsetWidth;

            if (event.id === 'event-system_sub') {
                // Move event-system_sub 3% to the left
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px - 3%)`;

                // Adjust the vertical line specifically for event-system_sub
                const style = document.createElement('style');
                style.innerHTML = `
                    #${event.id}::after {
                        left: 90%;
                    }
                `;
                document.head.appendChild(style);

            } else if (event.id === 'event-paper_sub') {
                // Move Prelim Submission (event-paper_sub) 3% to the left
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px - 3%)`;

                // Adjust the vertical line specifically for event-paper_sub
                const style = document.createElement('style');
                style.innerHTML = `
                    #${event.id}::after {
                        left: 90%;
                    }
                `;
                document.head.appendChild(style);

            } else if (event.id === 'event-paper_sub_final') {
                // Move Final Submission (event-paper_sub_final) 3% to the left
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px - 2%)`;

                // Adjust the vertical line specifically for event-paper_sub_final
                const style = document.createElement('style');
                style.innerHTML = `
                    #${event.id}::after {
                        left: 75%;
                    }
                `;
                document.head.appendChild(style);

            } else if (event.id === 'event-cam') {
                // Move Camera Ready (event-cam) 3% to the right
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px + 3%)`;

                // Adjust the vertical line specifically for event-cam
                const style = document.createElement('style');
                style.innerHTML = `
                    #${event.id}::after {
                        left: 10%;
                    }
                `;
                document.head.appendChild(style);

            } else if (event.id === 'event-notif') {
                // Move event-notif 3% to the right
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px + 3%)`;

                // Adjust the vertical line specifically for event-notif
                const style = document.createElement('style');
                style.innerHTML = `
                    #${event.id}::after {
                        left: 10%;
                    }
                `;
                document.head.appendChild(style);

            } else {
                // Standard centering for other events
                eventElement.style.left = `calc(${positionPercentage}% - ${eventBoxWidth / 2}px)`;
            }
        }
    });



    

    // Current Date Marker
    const currentDate = new Date();
    if (currentDate >= timelineStart && currentDate <= timelineEnd) {
        const elapsedDays = (currentDate - timelineStart) / (1000 * 60 * 60 * 24); // Days elapsed since start
        const positionPercentage = (elapsedDays / totalDurationInDays) * 100;
        const currentMarker = document.getElementById('current-date');
        currentMarker.style.left = `${positionPercentage}%`;
    }

    const aclStartDate = new Date('2025-07-27');
    const aclEndDate = new Date('2025-08-01');
    const aclStartDays = (aclStartDate - timelineStart) / (1000 * 60 * 60 * 24); // Days since timeline start
    const aclEndDays = (aclEndDate - timelineStart) / (1000 * 60 * 60 * 24); // Days since timeline start
    const aclStartPercentage = (aclStartDays / totalDurationInDays) * 100;
    const aclEndPercentage = (aclEndDays / totalDurationInDays) * 100;
    const aclRegionElement = document.getElementById('acl-region');

    if (aclRegionElement) {
        aclRegionElement.style.left = `${aclStartPercentage}%`;
        aclRegionElement.style.width = `${aclEndPercentage - aclStartPercentage}%`;
    }


    // Collapsible Sections
    const collapsibles = document.querySelectorAll(".collapsible");
    collapsibles.forEach(button => {
        button.addEventListener("click", () => {
            button.classList.toggle("active");
            const content = button.nextElementSibling;
            const plusMinus = button.querySelector(".plus-minus");
            if (content.style.display === "block") {
                content.style.display = "none";
                plusMinus.textContent = "+";
            } else {
                content.style.display = "block";
                plusMinus.textContent = "−";
            }
        });
    });

     // Collapsible Sections
    const collapsibles2 = document.querySelectorAll(".collapsible-2");
    collapsibles2.forEach(button => {
        button.addEventListener("click", () => {
            button.classList.toggle("active");
            const content = button.nextElementSibling;
            const plusMinus = button.querySelector(".plus-minus-2");
            if (content.style.display === "block") {
                content.style.display = "none";
                plusMinus.textContent = "+";
            } else {
                content.style.display = "block";
                plusMinus.textContent = "−";
            }
        });
    });

    // Function to open the References section and scroll to a specific reference
    function openReferencesAndScroll(refId) {
        // Get the references collapsible and content
        const referencesContent = document.getElementById('references-content');
        const collapsible = document.getElementById('references-section');

        // Check if the References collapsible container exists
        if (!referencesContent || !collapsible) {
            console.error('References section or content is missing in the DOM.');
            return; // Stop execution if the elements don't exist
        }

        // Open the collapsible container if it's closed
        if (referencesContent.style.display === 'none' || !referencesContent.style.display) {
            referencesContent.style.display = 'block'; // Show the references content
            collapsible.classList.add('active'); // Mark the collapsible as active

            // Update the plus-minus toggle symbol
            const plusMinus = collapsible.querySelector('.plus-minus');
            if (plusMinus) {
                plusMinus.textContent = '−'; // Update to a minus sign
            }
        }

        // Scroll to the specific reference
        const reference = document.getElementById(refId);
        if (reference) {
            reference.scrollIntoView({ behavior: 'smooth', block: 'start' });
        } else {
            console.error(`Reference with ID "${refId}" not found.`);
        }
    }

    // Add click event listeners to all links referencing the References section
    document.querySelectorAll('a[href^="#ref"]').forEach((link) => {
        link.addEventListener('click', (e) => {
            e.preventDefault(); // Prevent the default anchor link behavior
            const refId = link.getAttribute('href').substring(1); // Extract the reference ID (e.g., 'ref1')
            openReferencesAndScroll(refId); // Open and scroll to the reference
        });
    });

    // Function to toggle all collapsible sections
    function toggleAllSections() {
        const button = document.getElementById('expand-collapse-btn');
        const collapsibles = document.querySelectorAll('.collapsible');
        const contents = document.querySelectorAll('.content');

        if (button.textContent === "Expand All") {
            contents.forEach(content => content.style.display = "block");
            collapsibles.forEach(collapsible => {
                collapsible.classList.add("active");
                const plusMinus = collapsible.querySelector(".plus-minus");
                if (plusMinus) {
                    plusMinus.textContent = "−";
                }
            });
            button.textContent = "Collapse All";
        } else {
            contents.forEach(content => content.style.display = "none");
            collapsibles.forEach(collapsible => {
                collapsible.classList.remove("active");
                const plusMinus = collapsible.querySelector(".plus-minus");
                if (plusMinus) {
                    plusMinus.textContent = "+";
                }
            });
            button.textContent = "Expand All";
        }
    }

    // Add event listener to the Expand/Collapse button
    document.getElementById('expand-collapse-link').addEventListener('click', toggleAllSections);


</script>

</html>
